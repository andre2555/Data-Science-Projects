{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Customer-Queries-Classification-NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gofornaman/Data-Science-Projects/blob/master/Customer_Queries_Classification_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "i09FSfT-M_Yf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Say What? Customer Query Classification with some simple A.I\n",
        "\n",
        "![alt text](https://www.techbleez.com/wp-content/uploads/2018/06/Online-customer-support-with-AI.png)\n",
        "\n",
        "I´m happy to welcome you back to yet another short tutorial on how to use Machine Learning to help  accelerate and facilitate things in your company. Whatever sector you might be in, I´m certain that you have clients. And clients have this pesky tendency of complaining about stuff or requesting help ...\n",
        "\n",
        "Today´s exercise will show you how we can take a written message from a customer requesting customer support and classify that message as we see fit. The messages I am going to be using as an example today actually come from *real* customers from my very *real* website. Don´t worry, these messages are completely anonymized and do not contain any sensitive information. The website I run helps English teacher find easy classroom material, and everyday teachers write to us directly asking for specific material. Now in my case, each request we receive falls into one of two categories, it is either a request that can be dealt with automatically (category *0*) or it is something more detailed and complex and it needs manual, human intervention (category *1*). For your own case, the categories might vary, you could use urgent and non-urgent help requests, or even classify them into separate  areas ( category 0 messages for general support, category 1 messages goes to the sales teams, category 2 messages are for the fraud team, etc...). All depends on your needs and context. \n",
        "\n",
        "The idea here is for you to get an idea of how we can use A.I and M.L on raw text communication, so that it can inspire you to automate other parts of your company ( email classification, Chatbots, presentation summarizers etc., the list is endless!).\n",
        "\n",
        "The great thing for our use case is that the data is extremely basic: We just need the raw message from our message and its associated label. Have a look here at the [example dataset that we´ll be using here](https://docs.google.com/spreadsheets/d/1diEXKKl8j4SsqEopc7ln1NTZNzUZ6oFcN371XSkEoWo/edit#gid=0) from my users. So you can just scrape through all your pass customer service requests that will be enough for you to automate for the future.\n",
        "\n",
        "So here is, as always, the order of proceedings to get this mission accomplished:\n",
        "\n",
        "##Part 1: Data Cleaning ( As always the most important part)\n",
        "\n",
        "##Part 2 : Data Learning ( As always, the surprisingly easiest part)\n",
        "\n",
        "##Part 3: Data Predicting (As always the most fun part if all goes well)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "N12vRW2SpBcZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 0- Importing libraries\n",
        "\n",
        "But before all that, let´s import our libraries without which we couldn´t perform any magic tricks."
      ]
    },
    {
      "metadata": {
        "id": "vxSIfG6INDaW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A general purpose ML library that has almost everthing we need\n",
        "import sklearn\n",
        "from  sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import LinearSVC\n",
        "# for making linear algebra readable\n",
        "import numpy as np\n",
        "\n",
        "#Allows to upload and download files directly from the browser\n",
        "from google.colab import files \n",
        "\n",
        "\n",
        "#Allows us to manipulate data in a quick and easy way\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bq3s4iE0R1NR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1- Data Cleaning\n",
        "\n",
        "So let´s get started with the data pre-processing or \"cleaning\". This first step, before any actual Machine Learning has to be done, and it is usually the most crucial. Think of it as like cooking a meal, even if you are the best cook in the world, if our ingredients are going off or are missing, then our food will not taste well no matter what fancy utensils or techniques you use. On the flip side, if you have fresh, high quality ingredients, you can whip up a pretty delicious meal with pretty simply.\n",
        "\n",
        "The same analogy applies to M.L, if you don´t have well-processed data, then you can forget about the rest. First of all, let´s have a look at our dataset. As I mentioned before, we start off with two columns, the raw text input, and the second column giving us its category, being either 0 as a low-level request, and 1 being an urgent query that requires human intervention:"
      ]
    },
    {
      "metadata": {
        "id": "VufEOWF-iilo",
        "colab_type": "code",
        "outputId": "5a0e3db1-c47a-4678-b7e0-bb4ad38b9f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1949
        }
      },
      "cell_type": "code",
      "source": [
        "data_url=(\"https://raw.githubusercontent.com/busyML/Customer-Queries-Classification-NLP/master/NLP%20INSTANTEACH%20ML%20-%20Sheet1.csv\")\n",
        "\n",
        "data=pd.read_csv(data_url)\n",
        "\n",
        "data\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input Query</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>be or do</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>comparatives and superlatives</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how to teach overcoming obstacles concept wit...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Present perfect simple or continuous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-----</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1st conditional</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2018 World Cup</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018 World Cup</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2nd conditional</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>A career profile\\r\\nA descriptive article\\r\\nI...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>A listening exercise so an audio file must be ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>A lo of then</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>A lo of then</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>a new way to solve climate change. 1.company,2...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>A test paper covering Past simple present cont...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>abc, animals, school</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>abc, animals, school</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>about animals</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>About personal life, job and hobbies, family</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Abstract for iatofl and presentation on Englis...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Academic</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Academic writing practice</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Action Verbs Flashcards</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>adjective gerunds</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>adjectives</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Adjectives</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>adjectives, countries and nationalities</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Adverbs of frequency and WH-questions</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Adverbs, Comparativa of superiority</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>Crime and Prejudice</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>Book reviews</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>Julius Cesar</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>Einstein History</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1058</th>\n",
              "      <td>Darcy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1059</th>\n",
              "      <td>Jane Eyre for teenagers</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1060</th>\n",
              "      <td>Tv series</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1061</th>\n",
              "      <td>Jane Austin Literature</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1062</th>\n",
              "      <td>Surprise me this student has a good range of v...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1063</th>\n",
              "      <td>English Grammar</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>Sport Equipment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1065</th>\n",
              "      <td>festivals around the world</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1066</th>\n",
              "      <td>Scottish Accent</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1067</th>\n",
              "      <td>Scotland vocabulary</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1068</th>\n",
              "      <td>Wales country sheet</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1069</th>\n",
              "      <td>Economic Crisis Mexico 2008</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1070</th>\n",
              "      <td>Credit crunch</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>English Grammar</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1072</th>\n",
              "      <td>Present Progressive</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>friendship / LOVE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>Grammar and literature</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>Personal abbreviations</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1076</th>\n",
              "      <td>zero conditional if</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1077</th>\n",
              "      <td>all english subject</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1078</th>\n",
              "      <td>Final consonant sounds and general pronunciati...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1079</th>\n",
              "      <td>Writing</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>Murder</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1081</th>\n",
              "      <td>Predictions about the future</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082</th>\n",
              "      <td>Clauses</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>Twelfth night</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1084 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Input Query  Category\n",
              "0                                              be or do         0\n",
              "1                         comparatives and superlatives         0\n",
              "2      how to teach overcoming obstacles concept wit...         1\n",
              "3                  Present perfect simple or continuous         0\n",
              "4                                                     -         0\n",
              "5                                                 -----         0\n",
              "6                                       1st conditional         0\n",
              "7                                        2018 World Cup         1\n",
              "8                                        2018 World Cup         1\n",
              "9                                       2nd conditional         0\n",
              "10    A career profile\\r\\nA descriptive article\\r\\nI...         1\n",
              "11    A listening exercise so an audio file must be ...         0\n",
              "12                                         A lo of then         0\n",
              "13                                         A lo of then         0\n",
              "14    a new way to solve climate change. 1.company,2...         1\n",
              "15    A test paper covering Past simple present cont...         0\n",
              "16                                 abc, animals, school         0\n",
              "17                                 abc, animals, school         0\n",
              "18                                        about animals         0\n",
              "19         About personal life, job and hobbies, family         0\n",
              "20    Abstract for iatofl and presentation on Englis...         1\n",
              "21                                             Academic         0\n",
              "22                           Academic writing practice          0\n",
              "23                              Action Verbs Flashcards         1\n",
              "24                                    adjective gerunds         0\n",
              "25                                           adjectives         0\n",
              "26                                           Adjectives         0\n",
              "27              adjectives, countries and nationalities         0\n",
              "28                Adverbs of frequency and WH-questions         0\n",
              "29                  Adverbs, Comparativa of superiority         0\n",
              "...                                                 ...       ...\n",
              "1054                                Crime and Prejudice         1\n",
              "1055                                       Book reviews         1\n",
              "1056                                       Julius Cesar         1\n",
              "1057                                   Einstein History         1\n",
              "1058                                              Darcy         1\n",
              "1059                            Jane Eyre for teenagers         1\n",
              "1060                                          Tv series         1\n",
              "1061                             Jane Austin Literature         1\n",
              "1062  Surprise me this student has a good range of v...         1\n",
              "1063                                    English Grammar         0\n",
              "1064                                    Sport Equipment         1\n",
              "1065                         festivals around the world         1\n",
              "1066                                    Scottish Accent         1\n",
              "1067                                Scotland vocabulary         1\n",
              "1068                                Wales country sheet         1\n",
              "1069                        Economic Crisis Mexico 2008         1\n",
              "1070                                      Credit crunch         1\n",
              "1071                                    English Grammar         0\n",
              "1072                                Present Progressive         0\n",
              "1073                                  friendship / LOVE         1\n",
              "1074                             Grammar and literature         0\n",
              "1075                             Personal abbreviations         1\n",
              "1076                                zero conditional if         0\n",
              "1077                                all english subject         0\n",
              "1078  Final consonant sounds and general pronunciati...         1\n",
              "1079                                            Writing         0\n",
              "1080                                             Murder         1\n",
              "1081                       Predictions about the future         0\n",
              "1082                                            Clauses         0\n",
              "1083                                      Twelfth night         1\n",
              "\n",
              "[1084 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "vL9R-2HVIRK6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Always Shuffle\n",
        "\n",
        "So as we have seen in other tutorials, shuffling our data randomly should always be our first step. In our case, our data is ordered alphabetically, so we don´t want that to cause any unwanted biases to slip in. It’s always safest to shuffle our data before we even start touching it. And there is no excuse for not doing so when it can be done in 1 line of code:"
      ]
    },
    {
      "metadata": {
        "id": "NJCRYcaJ66SY",
        "colab_type": "code",
        "outputId": "5c0b518a-4401-438e-ad00-f9cff6e3da47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#we use the \"sample\" command of pandas to shuffle our data, the random state means that we will always shuffle the data in the same way so that when different people load this code, they will all get the same results.\n",
        "data= data.sample(frac=1, random_state=11)\n",
        "\n",
        "#we print out the first 11 rows of our data to check that it has indeed been shuffled, on the left we have the index number which we can also think of as an ID number.\n",
        "\n",
        "data.head(11)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input Query</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>640</th>\n",
              "      <td>reading, about 2 special schools in London and...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>grammar, functions, thematic reading, thematic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>Tenses</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>debate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>grammar . speaking.  writing</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>Places to go _ tourist</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>Camping</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>Texts and/or audios to expand practice on Simp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>Business</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>Sport</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>Be going to</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Input Query  Category\n",
              "640  reading, about 2 special schools in London and...         1\n",
              "336  grammar, functions, thematic reading, thematic...         0\n",
              "972                                            Tenses          0\n",
              "177                                             debate         0\n",
              "328                       grammar . speaking.  writing         0\n",
              "548                             Places to go _ tourist         0\n",
              "93                                            Camping          1\n",
              "782  Texts and/or audios to expand practice on Simp...         0\n",
              "80                                           Business          0\n",
              "750                                              Sport         0\n",
              "69                                         Be going to         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "vQi5dV_2q1GI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data.to_csv(\"shuffled.csv\")\n",
        "\n",
        "#files.download(\"shuffled.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "30Pq9iCyjSTA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 1- Vectorization of Words\n",
        "\n",
        "So there is a clear issue that you are probably wondering about, which is of course, how on earth can an algorithm read text? Well, the answer is, it doesn´t. As is always the case, we are going to have to find a way to convert our raw text to numbers so that our algorithm can learn from.\n",
        "\n",
        "Fortunately, this is a well-studied topic and there are countless of options we could use, ranging from the simplest to the most insanely complex.\n",
        "\n",
        "We´ll go through the simplest form, explain its limitation and then gives us a medium-range solution that should work well in most cases:\n",
        "\n",
        "\n",
        "***Counting Word Frequency***\n",
        "\n",
        "So this is the granddad of word vectorization, the simplest way we can transform words to number, for each word in a sentence, we count the number of times each word appears. When we are comparing different raw text, we´ll take the whole range of vocabulary and count for each sentence the frequency of for each word in our total vocabulary. This simplistic technique is commonly called the \"bag of words\" technique\n",
        "\n",
        "![alt text](https://www.python-course.eu/images/bag_of_words.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To see this in practice, [here is how the dataset would look after the transformation at the following link](https://docs.google.com/spreadsheets/d/1diEXKKl8j4SsqEopc7ln1NTZNzUZ6oFcN371XSkEoWo/edit#gid=1530603417). \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pOHbx4KqmMWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "count_vectorization = CountVectorizer()\n",
        "\n",
        "count_example= (count_vectorization.fit_transform(data[\"Input Query\"].values.astype('U'))).toarray()\n",
        "\n",
        "count_example =pd.DataFrame(count_example)\n",
        "\n",
        "vocab_list=list (count_vectorization.get_feature_names())\n",
        "\n",
        "i=0\n",
        "for i in range(len(count_example.columns)):\n",
        "  count_example.rename(columns={i: vocab_list[i]}, inplace=True)\n",
        "  \n",
        "\n",
        "#count_example.to_csv(\"countexample.csv\")\n",
        "\n",
        "#files.download(\"countexample.csv\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_sn2IXTszBT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "However, this way of converting the text really isn´t the best, sometimes the algorithm won´t be able to learn much from it because of its simplistic nature. But most importantly, this count vectorization generates a big big dataset and this can be a big problem. Take for instance our small dataset here is only of a thousand queries and there are 1,200 unique vocabulary words. 1000 x 1,200 =... Over a million cells! \n",
        "\n",
        "So even with just a small dataset of queries, our numerical dataset is huge. Now keeping in mind that a medium sized company will probably have at least 100,000 past examples with 10,000 of individual words in it and then the size truly becomes mind- boggling. We need a more practical system that can scale better.\n",
        "\n",
        "**Hashing Vectorization**\n",
        "\n",
        "So hashing vectorization uses a bit of good´ol math wizardry to plot all the words on a graph and then evaluates their proximity to another. This way, the algorithm can start understanding the relevance of one word to another and get a better, overall idea of how some words are interconnected. To wrap your head around this concept, have a look at the following representations:\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Scatter-Plot-of-PCA-Projection-of-Word2Vec-Model.png)\n",
        "\n",
        "As we can see, the algorithm looks at how words tend to cluster in different contexts and finds the relations between them based on how they were used in the text. Once, again [if you want to see how the dataset ended up looking, have a gander over at this link.](https://docs.google.com/spreadsheets/d/1diEXKKl8j4SsqEopc7ln1NTZNzUZ6oFcN371XSkEoWo/edit#gid=281324509)"
      ]
    },
    {
      "metadata": {
        "id": "gjTwMl84lQoE",
        "colab_type": "code",
        "outputId": "2a59042a-57a8-480d-9eaf-9833ba692a5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "cell_type": "code",
      "source": [
        "#We call the Hashing Vectorizer from SKlearn, and we limit its columns to 1024 (2 to the tenth power)\n",
        "\n",
        "vectorization= HashingVectorizer(n_features=2**10, norm = \"l1\")\n",
        "\n",
        "#applying the vectorizer on our text\n",
        "vec_counts = (vectorization.fit_transform(data[\"Input Query\"].values.astype('U'))).toarray()\n",
        "\n",
        "#putting our training data in Pandas format\n",
        "training_data=pd.DataFrame(vec_counts)\n",
        "\n",
        "#We create an excel file that contains the wine with their new categories\n",
        "#training_data.to_excel(\"instanteachnlptraining.xlsx\")\n",
        "\n",
        "#We use the \".download\" command to download the new excel file to our browser\n",
        "#files.download(\"instanteachnlptraining.xlsx\")\n",
        "\n",
        "training_data.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.027778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.027778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1     2     3     4     5     6     7     8         9     ...   1014  \\\n",
              "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0 -0.027778  ...    0.0   \n",
              "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  ...    0.0   \n",
              "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  ...    0.0   \n",
              "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  ...    0.0   \n",
              "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  ...    0.0   \n",
              "\n",
              "   1015  1016  1017  1018      1019  1020  1021  1022  1023  \n",
              "0   0.0   0.0   0.0   0.0 -0.027778   0.0   0.0   0.0   0.0  \n",
              "1   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
              "2   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
              "3   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
              "4   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0  \n",
              "\n",
              "[5 rows x 1024 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "sinwqPmp0_qd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Right, so after a few lines of code, we have our dataset. Most of the time when dealing with text input, this Hash vectorization will be the way to go. The great thing here is that we are able to limit the number of columns of the conversion generates meaning that our dataset is limited in size, it will always have around 1,000 columns, which means that adding more examples is less problematic. If you wish to use this code for yourself and you want to use something like 100,000 examples instead of only a 1,000 like I have, you might want to increase the \"n_features\" to 2^11 or 2^12, increasing the number of columns.\n",
        "\n",
        "However, although our dataset is now no longer going to grow exponentially, it is still rather big and maybe can still be difficult to scale depending on your computing resources. Furthermore, if you look at our dataset, you´ll see that there are many zeroes, which seems a bit unnecessary. If only there was a way to compress all these numbers so that it didn´t take so much space in our computer´s memory...\n",
        "\n",
        "**Compressing the data with PCA**\n",
        "\n",
        "There is a great technique for this called [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), or PCA as every normal mortal calls it. PCA takes any dataset and basically boils it down to a more concise version, keeping only the most essential data and discarding the rest, allowing the dataset to become smaller. It´s like when you take someone´s rambling 3 paragraph email and turn it into a few bullet points. Or another analogy could be of taking a high resolution picture and converting it to a lower quality; we´ll still be able to see and understand what the picture is despite it being a bit fuzzier, but the picture will take up less space on our hard drive: \n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/Benli11/data/master/img/reTiger.png)\n",
        "\n",
        "Of course, the second picture is of less quality **but we can still clearly identify it as a tiger**. That's the core idea behind PCA.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "E92V8CcJ0-py",
        "colab_type": "code",
        "outputId": "f4e59a89-7ead-47de-de70-0eb4aa1367eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "training_rows, training_columns= training_data.shape\n",
        "\n",
        "#We load PCA from Sklearn, the \"0.99\" means that we want the compression to retain 99% of the original data´s significance. \n",
        "pca_compressor=PCA(0.99)\n",
        "\n",
        "#We use the PCA transformer to compress the image and we use the \"DataFrame\" function to convert it into our favorite pandas format\n",
        "compressed_training_data = pd.DataFrame(pca_compressor.fit_transform(training_data))\n",
        "\n",
        "#we get how many columns the new compressed dataset has so that we can compate with the original one.\n",
        "compressed_rows, compressed_columns = compressed_training_data.shape\n",
        "\n",
        "#We  print out the comparison between the two\n",
        "print(\"number of columns before compression:\",training_columns,\"\\n\",\"number of columns after compression:\" ,compressed_columns)\n",
        "\n",
        "#compressed_training_data.to_excel(\"pcadata.xlsx\")\n",
        "\n",
        "#files.download(\"pcadata.xlsx\")\n",
        "\n",
        "compressed_training_data.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of columns before compression: 1024 \n",
            " number of columns after compression: 371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>361</th>\n",
              "      <th>362</th>\n",
              "      <th>363</th>\n",
              "      <th>364</th>\n",
              "      <th>365</th>\n",
              "      <th>366</th>\n",
              "      <th>367</th>\n",
              "      <th>368</th>\n",
              "      <th>369</th>\n",
              "      <th>370</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.018548</td>\n",
              "      <td>-0.009943</td>\n",
              "      <td>-0.041752</td>\n",
              "      <td>-0.009834</td>\n",
              "      <td>0.038122</td>\n",
              "      <td>-0.020786</td>\n",
              "      <td>0.032670</td>\n",
              "      <td>-0.002762</td>\n",
              "      <td>-0.022673</td>\n",
              "      <td>0.002803</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.002007</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.005563</td>\n",
              "      <td>0.005353</td>\n",
              "      <td>0.003982</td>\n",
              "      <td>-0.002125</td>\n",
              "      <td>0.005118</td>\n",
              "      <td>-0.002865</td>\n",
              "      <td>-0.002994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.144641</td>\n",
              "      <td>-0.030201</td>\n",
              "      <td>-0.024569</td>\n",
              "      <td>-0.010797</td>\n",
              "      <td>0.046357</td>\n",
              "      <td>0.016590</td>\n",
              "      <td>0.104466</td>\n",
              "      <td>-0.018984</td>\n",
              "      <td>-0.016666</td>\n",
              "      <td>0.089007</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>-0.004897</td>\n",
              "      <td>-0.001377</td>\n",
              "      <td>-0.005838</td>\n",
              "      <td>-0.000881</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.004190</td>\n",
              "      <td>0.005479</td>\n",
              "      <td>-0.001715</td>\n",
              "      <td>-0.003087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.036206</td>\n",
              "      <td>-0.031946</td>\n",
              "      <td>0.059021</td>\n",
              "      <td>-0.153236</td>\n",
              "      <td>0.376132</td>\n",
              "      <td>0.819627</td>\n",
              "      <td>-0.134380</td>\n",
              "      <td>-0.002157</td>\n",
              "      <td>0.215932</td>\n",
              "      <td>-0.243564</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.000047</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.001010</td>\n",
              "      <td>0.001648</td>\n",
              "      <td>-0.000380</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.001204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.017365</td>\n",
              "      <td>-0.006946</td>\n",
              "      <td>-0.032016</td>\n",
              "      <td>-0.010879</td>\n",
              "      <td>0.022846</td>\n",
              "      <td>-0.024185</td>\n",
              "      <td>-0.008863</td>\n",
              "      <td>0.004951</td>\n",
              "      <td>-0.010268</td>\n",
              "      <td>-0.015265</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.000030</td>\n",
              "      <td>-0.000060</td>\n",
              "      <td>0.000081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.311567</td>\n",
              "      <td>-0.059424</td>\n",
              "      <td>-0.079415</td>\n",
              "      <td>0.240529</td>\n",
              "      <td>-0.139072</td>\n",
              "      <td>0.096719</td>\n",
              "      <td>-0.001947</td>\n",
              "      <td>0.005382</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>0.014510</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002816</td>\n",
              "      <td>-0.003351</td>\n",
              "      <td>0.000699</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>-0.001988</td>\n",
              "      <td>-0.002830</td>\n",
              "      <td>-0.000235</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 371 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0 -0.018548 -0.009943 -0.041752 -0.009834  0.038122 -0.020786  0.032670   \n",
              "1  0.144641 -0.030201 -0.024569 -0.010797  0.046357  0.016590  0.104466   \n",
              "2 -0.036206 -0.031946  0.059021 -0.153236  0.376132  0.819627 -0.134380   \n",
              "3 -0.017365 -0.006946 -0.032016 -0.010879  0.022846 -0.024185 -0.008863   \n",
              "4  0.311567 -0.059424 -0.079415  0.240529 -0.139072  0.096719 -0.001947   \n",
              "\n",
              "        7         8         9      ...          361       362       363  \\\n",
              "0 -0.002762 -0.022673  0.002803    ...     0.000165  0.002007  0.000524   \n",
              "1 -0.018984 -0.016666  0.089007    ...     0.000126 -0.004897 -0.001377   \n",
              "2 -0.002157  0.215932 -0.243564    ...     0.000546 -0.000513 -0.000047   \n",
              "3  0.004951 -0.010268 -0.015265    ...     0.000110  0.000158  0.000085   \n",
              "4  0.005382 -0.000205  0.014510    ...    -0.002816 -0.003351  0.000699   \n",
              "\n",
              "        364       365       366       367       368       369       370  \n",
              "0  0.005563  0.005353  0.003982 -0.002125  0.005118 -0.002865 -0.002994  \n",
              "1 -0.005838 -0.000881  0.003819  0.004190  0.005479 -0.001715 -0.003087  \n",
              "2  0.000207 -0.000240 -0.001010  0.001648 -0.000380  0.000367  0.001204  \n",
              "3  0.000037 -0.000088  0.000028  0.000011 -0.000030 -0.000060  0.000081  \n",
              "4  0.001242  0.000354  0.000023  0.001958 -0.001988 -0.002830 -0.000235  \n",
              "\n",
              "[5 rows x 371 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "SYGxtra87_U6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Well, would you believe it! After applying the compression process, we only have 371 columns in our dataset, in other words we´ve reduced the size of the dataset by 65% and all losing 1% significance of the original data. As you can see, we no longer have sparse zero columns, everything has been compressed as tightly as possible in order to have as few columns as possible. This now makes our data far more scalable when adding examples to it in the future.\n",
        "\n",
        "### The \"answer\" dataset\n",
        "\n",
        "Well we now have our training data ready, all we need now is to create another, much smaller dataset that stores the attributed label, and we can do so with one line of code. Our algorithm will use this as its basis to learn and classify future queries."
      ]
    },
    {
      "metadata": {
        "id": "iumUSagmwyCV",
        "colab_type": "code",
        "outputId": "917748e4-3f3d-4206-e66b-51bfe60f72b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "cell_type": "code",
      "source": [
        "training_answers= data[\"Category\"]\n",
        "\n",
        "training_answers.head(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "640    1\n",
              "336    0\n",
              "972    0\n",
              "177    0\n",
              "328    0\n",
              "548    0\n",
              "93     1\n",
              "782    0\n",
              "80     0\n",
              "750    0\n",
              "69     0\n",
              "792    0\n",
              "672    1\n",
              "38     0\n",
              "950    1\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "jIkhOwiT2y0p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our model will use these answers to \"learn\" the relation between each input text and it´s label category.\n",
        "\n",
        "Of course, in your context, you might have several different categories assigned to your customer queries. The models should be able to handle multi queries as well if you so need it. "
      ]
    },
    {
      "metadata": {
        "id": "6dN2d34A3X8Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning from the data\n",
        "\n",
        "So now the “surprisingly easy\" part. Indeed, if our data has been well cleaned and formatted in the previous step and it should be relatively easy to get good results. First of all, this is a classification problem, so the first thing that should jump to our mind is: what´s more dangerous for my company? A false positive or a false negative? Enter the concepts of Precision and Recall:\n",
        "\n",
        "###Step 1- Choosing between Precision or Recall as our key metric:\n",
        "\n",
        "First of all, let's define some useful vocabulary. This will help us to understand how well our model is performing. There are four types of predictions that our model could make: True Positive, False Positive, True Negative, False Negative. Let's define these terms:\n",
        "\n",
        "True Positive: This is a good prediction, our model predicted that the customer was going to churn (emitted a 1) and the customer did in reality churn (churn=1).\n",
        "\n",
        "False Positive: This is an incorrect prediction, our model predicted that our customer churned ( emitted a 1), however it turns out that customer did not in fact churn (churn=0)\n",
        "\n",
        "True Negative : This is again a correct prediction, our model said that the customer was not going to churn ( it emitted a 0) and this turned out to be correct in the real world ( churn was equal to 0)\n",
        "\n",
        "False Negative: Again, this is an incorrect prediction, but of a different kind. Here our model predicted that the customer was not going to churn (emitted a 0), but lo and behold, in reality our customer did in fact leave the company (churn=1)\n",
        "\n",
        "With this now clearly defined, we need to decide what evaluation metric we will use to evaluate our model.\n",
        "\n",
        "This actually is a crucial concept, and we have three evaluation options to choose from:\n",
        "\n",
        "\n",
        "*   **Precision** - High precision means that the model does not give many \"False Positives\".\n",
        "\n",
        "*   **Recall** High Recall means that model has a very low proportion of \"False Negatives\".\n",
        "\n",
        "* **F1 Score **  The F1 Score is a balance between Precision and Recall, this a great measure for overall accuracy.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)\n",
        "\n",
        "\n",
        "Which of these metrics are most important to pay attention to? Well, this wholly depends on your context and what type of error is most costly to your business. Here we need to analyze the consequence of each type of error.\n",
        "\n",
        "If, for example a false positive is very very costly, such as in the case of Face Recognition or Mortgage Approval, then we will want to maximise the Precision metric as much as possible.\n",
        "\n",
        "However, if getting a false negative is even more problematic, as for example in cases such as Fraud Detection or Cancer Diagnosis, then we'll make sure our model has the highest Recall measure possible.\n",
        "\n",
        "If obtaining a false negative is just as bad as a false positive and there is no difference between them, then we can simply use the F1 score.\n",
        "\n",
        "Remember there is no right or wrong answer here because it is a judgment call based on your own individual context.\n",
        "\n",
        "So for the personal context of my website, it just so happens that it is far more important for me to be able to detect the \"1\" category because these are urgent and need human intervention, whereas the \"0\" category responses are not so important since they can be dealt with automatically by the system. It´s not a big deal if a few \"0\" type queries get bunched with the \"1\", a human can also deal with them. However, if a user is issuing an urgent query that demands human attention and the model classifies that as being a non-important \"0\" query, then that could cause high customer dissatisfaction (because no one will be dealing with their query).\n",
        "\n",
        "Therefore in practice, I will want my model to be \"paranoid\" and to always lean towards the \"1\" category unless it is extremely certain that it is a type \"0\" query. Basically, if there when there is any room for doubt, the model will classify the query as being of a \"1\" just to be on the safe side of things.\n",
        "\n",
        "Of course this decision is based on my unique business context, you will have to go through this decision-making process yourself for your own company needs, but it´s nothing that a bit of common sense can´t handle.\n",
        "\n",
        "So how do we manage this programmatically? We simply need to assign to each category, in our case \"0\" and \"1\", different weights. We´ll assign a heavier weight to the \"1\" category because we want to be as certain as possible to always detect that class. If you want to use this code for yourself, you can adjust the weights for your own classes to what you need for your own use case.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3RlwmRQXOXKL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the class weights tell the algorithm how paranoid to be about each class. In this case, the 0.87 for the \"1\" category means that it will only categorize a query as \"0\" if it is more than 87% sure. Otherwise it will classify it as \"1\", which means that it´s not taking any risks with the \"1\" category\n",
        "class_weights = {0:0.13, 1:0.87}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5WtUwDprWnL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2- Learning about the learning algorithm\n",
        "\n",
        "Right, now let´s get to the fancy part. We´ll be using an algorithm called a linear SVC (Support Vector Classifier). It´s not too complex of a concept as I hope you can discern from the beautiful drawings below. And as always, you don´t need to know the math behind it, all you need to know is the one line of code to load it from the Sklearn library.\n",
        "\n",
        "![alt text](http://michelleful.github.io/code-blog/assets/images/201506/svm2_new.png)\n",
        "\n",
        "![alt text](https://chrisalbon.com/images/machine_learning_flashcards/Support_Vector_Classifier_print.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AhX7S17kja_O",
        "colab_type": "code",
        "outputId": "2762bd30-9ffc-4c85-f76c-787f3dd13abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# We load the LinearSVC from Scikit learn. We use the C parameter from stopping the model jsut memorizing the dataset, then we also input the clas weights... in the class weights parameter\n",
        "svc_model= LinearSVC(C=7, dual=True, loss=\"squared_hinge\", penalty=\"l2\", tol=1e-7, class_weight=class_weights)\n",
        "\n",
        "#we use the .fit command to get the model to learn from the formatted compressed data, matching them with the training answers. \n",
        "svc_model.fit(compressed_training_data,training_answers)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=7, class_weight={0: 0.13, 1: 0.87}, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "     multi_class='ovr', penalty='l2', random_state=None, tol=1e-07,\n",
              "     verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "s4NdLJURvXbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As always we can load the model with one simple line of code and train it with the \".fit\" command to get to learn from the data. Learning from the data will only take a few seconds! As you can see from the code below, it is very easy and simple to implement:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hRgRDaq_RlWA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 3 - Evaluating the model\n",
        "\n",
        "So now we actually need to test our model, to see if this thing actually works! Below I´ve constructed a little program that counts the number of false positives and the number of false negatives etc. that our model generates. \n",
        "\n",
        "For the eagle-eyed out there, you will have noticed that we are not using \"test\" dataset here to examine the performance of our model, but rather, we are re-using out training data for this. How dare we?! The reason is that we are in the online context, where this model will always be tested and learning in real time as user input comes through. The true test will come when we deploy into the real world. This also means we´ll be adding to our dataset new examples ideally every couple of weeks and that it will steadily and the model will be improving over time. \n",
        "\n",
        "So below, we´ll count the number of correct and incorrect answers and we´ll print out our key metrics:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SrhZlAuVj5XN",
        "colab_type": "code",
        "outputId": "829d6577-1c30-4063-e108-48da2bff4b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "#initializing our counts of true positive, false negative, etc.\n",
        "TP=0\n",
        "TN=0\n",
        "FP=0\n",
        "FN=0\n",
        "\n",
        "#we create what is called a for loop to iterate through every row of the dataset\n",
        "for i in range(len(training_data)):\n",
        "    #Counts True Positives if the answer is \"1\" and the model predicted \"1\"\n",
        "    if svc_model.predict(compressed_training_data.iloc[[i,]])==1 and training_answers.iloc[i]==1:\n",
        "                                       TP=TP+1\n",
        "        \n",
        "    ##Counts False Positives if the answer is \"0\" and the model predicted \"1\"\n",
        "    if svc_model.predict(compressed_training_data.iloc[[i,]])==1 and training_answers.iloc[i]==0:\n",
        "                                       FP=FP+1    \n",
        "    ##Counts True Negatives if the answer is \"0\" and the model predicted \"0\"\n",
        "    if svc_model.predict(compressed_training_data.iloc[[i,]])==0 and training_answers.iloc[i]==0:\n",
        "                                       TN=TN+1    \n",
        "    \n",
        "    #Counts False Negatives if the answer is \"1\" and the model predicted \"0\"\n",
        "    if svc_model.predict(compressed_training_data.iloc[[i,]])==0 and training_answers.iloc[i]==1:\n",
        "                                       FN=FN+1   \n",
        "\n",
        "print (\"Model\", \"True Positives:\",TP, \"False Positives:\",FP,\"True Negatives:\",TN , \"False Negatives:\", FN) \n",
        "\n",
        "model_accuracy= (TP+TN)/(len(training_answers))\n",
        "model_precision= TP/(TP+FP) \n",
        "model_recall=TP/(TP+FN)\n",
        "model_f1= 2 * (model_precision * model_recall) / (model_precision + model_recall)\n",
        "\n",
        "print(\"Model´s Accuracy On Training Data:\", model_accuracy *100,'%')\n",
        "print(\"Model´s Precision On Training Data:\", model_precision *100,'%')\n",
        "print(\"Model´s Recall On Training Data:\", model_recall *100,'%')\n",
        "print(\"Model´s F1 On Training Data:\", model_f1 *100,'%')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model True Positives: 191 False Positives: 125 True Negatives: 767 False Negatives: 1\n",
            "Model´s Accuracy On Training Data: 88.37638376383764 %\n",
            "Model´s Precision On Training Data: 60.44303797468354 %\n",
            "Model´s Recall On Training Data: 99.47916666666666 %\n",
            "Model´s F1 On Training Data: 75.1968503937008 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "48JTORkPsTyO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So we´ve calculated our metrics by using some pretty simple sumation and multiplication. \n",
        "\n",
        "And as we can see, the most important metric for us, **Recall** was pretty good, with 99% (note that this will probably drop in the real world to maybe around 90-95%). We also see that we have quite a few false positives but that´s ok for our business context. Of course, if in your case you wanted to prioritize the “0\" category, then the **precision** metric is the one you would try to maximize (by readjusting the class weights from earlier.)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4PDK-vTHtxOh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3- Data Predicting \n",
        "\n",
        "So we've done the hardest part, now we need to create a little program that allows us to classify any new text inputted into it. The idea here is that you'll have this program waiting in the background on your platform and whenever a customer issues a customer support query, then the program will classify query and direct it to correct funnel for it to be solved.\n",
        "\n",
        "This section is more to do with developer implementation and deployment, so it's not so important to look through the code. However, I do recommend that you have a go at it yourself and input some text as an example. (For this, pretend that you are an English teacher who needs a specific topic for your class. A general query should output a 0 whereas a specific query should be classified as a 1.) Have fun!"
      ]
    },
    {
      "metadata": {
        "id": "CggYEIW6kbSH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def query_classifier(new_input):\n",
        "  #we get the raw text into a \"list\" format with these brackets\n",
        "  new_input=[new_input]\n",
        "\n",
        "  #Now we need to format it in the same way we formated our training data. we first apply to it the hash vectorization to get it into the same format\n",
        "  new_input_vectorized = vectorization.fit_transform(new_input)\n",
        "  new_input_vectorized=pd.DataFrame(new_input_vectorized.toarray())\n",
        "  \n",
        "  #Now that we have the hash vectors, we can compress it using PCA (we actually need to add to the training set because PCA compresses the data in function of other data)\n",
        "  compressing_new_input= training_data.append(new_input_vectorized, ignore_index=True)\n",
        "  pca_input_compressor= PCA(n_components=compressed_columns, svd_solver='full')\n",
        "  \n",
        "  #We compress the data...\n",
        "  compressing_new_input= pd.DataFrame(pca_input_compressor.fit_transform(compressing_new_input))\n",
        "  \n",
        "  #And now we extract the last row that corresponds to the last row which is our new formatted input that we want to predict\n",
        "  new_input_compressed = compressing_new_input.iloc[[(len(compressing_new_input.index)-1),]]\n",
        "\n",
        "  #Now we use the \".predict\" function to classify the text as \"0\" or \"1\"\n",
        "  prediction=svc_model.predict(new_input_compressed)\n",
        "  \n",
        "  if prediction==0:\n",
        "    print(prediction)\n",
        "    print(\"Not to worry, we can deal with this query automatically. This is not an urgent request!\")\n",
        "  else:\n",
        "    print(prediction)\n",
        "    print(\"Human, help please! This request is too complex and specific... Please do it manually\")\n",
        "    \n",
        "\n",
        "new_input= (input(\"input new text:\"))\n",
        "\n",
        "\n",
        "query_classifier(new_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MTEJBEnMEylW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "That's all. I hope you were able to appreciate how easy it was to automatic a significant portion of the customer support query helpline. Once we know what type of query is being inputted, we can do lots of magical things like take the user to the exact help page he/she needs or open a chat with a human agent all depending on what they wrote in their support query and the problem we are having. I can only see this as a win-win seeing as the user gets a more personalized and satisfying experience meanwhile the company saves precious human time.\n",
        "\n",
        "So that will be it for today. The key things to remember is that by using a **\"hash vectorizer\"**, we can conveniently convert any raw text to numbers that can then be easily digested by the algorithm. However, depending on your case, this text-to-number conversion can generate huge datasets, so we can use PCA compression to reduce it and save valuable memory space.\n",
        "\n",
        "Lastly and perhaps the most important message of all is that I hope you continue to be convinced that you don't need to be a programmer to implement these tools in your company today and you can do so to make your and your colleagues’' lives a lot easier.\n",
        "\n",
        "Feel free to contact me about any questions, comments or feedback at my email: [conrad.w.s@gmail.com](mailto:conrad.w.s@gmail.com) or hit me up [on Linkedin at Conrad WS.](https://www.linkedin.com/in/conrad-wilkinson-schwarz-210aa9b2/)"
      ]
    },
    {
      "metadata": {
        "id": "GDSYx7VSGjIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"thank you for reading!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}